import {
    DeepSeekFinishReasonEnum,
    DeepSeeKModelsEnum,
    DeepSeekObjectEnum,
    DeepSeekResponseFormatTypeRequestEnum,
    DeepSeekToolCallTypeEnum,
    DeepSeekToolChoiceMessageRoleResponseEnum,
    DeepSeekToolchoiceRequestEnum,
    DeepSeekToolMessageRoleRequestEnum,
} from '../enums/deepseek.enum';

/**
 * The response api for the deepseek chat completion.
 */
export interface IDeepSeekResponse {
    /**
     * A unique identifier for the chat completion.
     */
    id: string;

    /**
     * The object type, which is always chat.completion
     */
    object: DeepSeekObjectEnum;

    /**
     * The Unix timestamp (in seconds) of when the chat completion was created.
     */
    created: number;

    /**
     * The model used for the chat completion.
     */
    model: DeepSeeKModelsEnum;

    /**
     * A list of chat completion choices.
     */
    choices: IDeepSeekChoiceResponse[];

    /**
     * Usage statistics for the completion request.
     */
    usage: IDeepSeekUsageResponse;

    /**
     * This fingerprint represents the backend configuration that the model runs with.
     */
    system_fingerprint: string;
}

export interface IDeepSeekChoiceResponse {
    /**
     * The index of the choice in the list of choices.
     */
    index: number;

    /**
     * A chat completion message generated by the model.
     */
    message: {
        /**
         * The role of the author of this message.
         */
        role: DeepSeekToolChoiceMessageRoleResponseEnum;

        /**
         * The contents of the message.
         */
        content?: string;

        /**
         * For deepseek-reasoner model only.
         * The reasoning contents of the assistant message, before the final answer.
         */
        reasoning_content?: string;

        /**
         * The tool calls generated by the model, such as function calls.
         */
        tool_calls?: IDeepSeekChoiceToolCallResponse[];
    };

    /**
     * Log probability information for the choice.
     */
    logprobs?: IDeepSeekChoiceLogprobsResponse;
    finish_reason: DeepSeekFinishReasonEnum;
}

export interface IDeepSeekChoiceToolCallResponse {
    id: string;

    /**
     * The type of the tool. Currently, only function is supported.
     */
    type: DeepSeekToolCallTypeEnum;

    /**
     * The function that the model called.
     */
    funtion: IDeepSeekChoiceToolCallFunctionResponse;
}

export interface IDeepSeekChoiceToolCallFunctionResponse {
    /**
     * The ID of the tool call.
     */
    id: string;
    /**
     * The name of the function to call.
     */
    name: string;

    /**
     * The arguments to call the function with, as generated by the model in JSON format.
     * Note that the model does not always generate valid JSON,
     * and may hallucinate parameters not defined by your function schema.
     * Validate the arguments in your code before calling your function.
     */
    arguments: string;
}

export interface IDeepSeekChoiceLogprobsResponse {
    /**
     * A list of message content tokens with log probability information.
     */
    content?: IDeepSeekChoiceLogprobsContentResponse[];
}

export interface IDeepSeekChoiceLogprobsContentResponse {
    /**
     * The token.
     */
    token: string;

    /**
     * The log probability of this token, if it is within the top 20 most likely tokens.
     * Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
     */
    logprob: number;

    /**
     * A list of integers representing the UTF-8 bytes representation of the token.
     * Useful in instances where characters are represented by multiple tokens
     * and their byte representations must be combined to generate the correct text representation.
     * Can be null if there is no bytes representation for the token.
     */
    bytes?: number;

    /**
     * List of the most likely tokens and their log probability, at this token position.
     * In rare cases, there may be fewer than the number of requested top_logprobs returned.
     */
    top_logprobs: IDeepSeekChoiceLogprobsContentTopLogprobsResponse[];
}

export interface IDeepSeekChoiceLogprobsContentTopLogprobsResponse {
    /**
     * The token.
     */
    token: string;

    /**
     * The log probability of this token, if it is within the top 20 most likely tokens.
     * Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
     */
    logprob: number;

    /**
     * A list of integers representing the UTF-8 bytes representation of the token.
     * Useful in instances where characters are represented by multiple tokens
     * and their byte representations must be combined to generate the correct text representation.
     * Can be null if there is no bytes representation for the token.
     */
    bytes?: number;
}

export interface IDeepSeekUsageResponse {
    /**
     * Number of tokens in the generated completion.
     */
    completion_tokens: number;

    /**
     * Number of tokens in the prompt.
     * It equals prompt_cache_hit_tokens + prompt_cache_miss_tokens.
     */
    prompt_tokens: number;

    /**
     * Number of tokens in the prompt that hits the context cache.
     */
    prompt_cache_hit_tokens: number;

    /**
     * Number of tokens in the prompt that misses the context cache.
     */
    prompt_cache_miss_tokens: number;

    /**
     * Total number of tokens used in the request (prompt + completion).
     */
    total_tokens: number;

    /**
     * Breakdown of tokens used in a completion.
     */
    completion_tokens_details: IDeepSeekUsageCompletionTokensDetailsResponse;
}

export interface IDeepSeekUsageCompletionTokensDetailsResponse {
    /**
     * Tokens generated by the model for reasoning.
     */
    reasoning_tokens: number;
}

/**
 * The request api for the deepseek chat completion.
 */
export interface IDeepSeekRequest {
    /**
     * A list of messages comprising the conversation so far.
     */
    messages: IDeepSeekMessageRequest[];

    /**
     * ID of the model to use. You can use deepseek-chat.
     */
    model: DeepSeeKModelsEnum;

    /**
     * Default value: 0
     *
     * Number between -2.0 and 2.0.
     * Positive values penalize new tokens based on their existing frequency
     * in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     */
    frecuency_penalty?: number;

    /**
     * Integer between 1 and 8192. The maximum number of tokens that can be generated in the chat completion.
     * The total length of input tokens and generated tokens is limited by the model's context length.
     * If max_tokens is not specified, the default value 4096 is used.
     */
    max_tokens?: number;

    /**
     * Default value: 0
     *
     * Number between -2.0 and 2.0.
     * Positive values penalize new tokens based on whether they appear in the text so far,
     * increasing the model's likelihood to talk about new topics.
     */
    presence_penalty?: number;

    /**
     * An object specifying the format that the model must output.
     * Setting to { "type": "json_object" } enables JSON Output,
     * which guarantees the message the model generates is valid JSON.
     *
     * Important: When using JSON Output,
     * you must also instruct the model to produce JSON yourself via a system or user message.
     * Without this, the model may generate an unending stream of whitespace until
     * the generation reaches the token limit, resulting in a long-running
     * and seemingly "stuck" request.
     * Also note that the message content may be partially cut off if finish_reason="length",
     * which indicates the generation exceeded max_tokens
     * or the conversation exceeded the max context length.
     */
    response_format?: IDeepSeekResponseFormatRequest;

    /**
     * Up to 16 sequences where the API will stop generating further tokens.
     */
    stop?: string | string[];

    /**
     * If set, partial message deltas will be sent.
     * Tokens will be sent as data-only server-sent events (SSE) as they become available,
     * with the stream terminated by a data: [DONE] message.
     */
    stream?: boolean;

    /**
     * Options for streaming response. Only set this when you set stream: true.
     */
    stream_options?: IDeepSeekStreamOptionsRequest;

    /**
     * Default value: 1
     *
     * What sampling temperature to use, between 0 and 2.
     * Higher values like 0.8 will make the output more random,
     * while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or top_p but not both.
     */
    temperature?: number;

    /**
     * Default value: 1
     *
     * An alternative to sampling with temperature, called nucleus sampling,
     * where the model considers the results of the tokens with top_p probability mass.
     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    top_p?: number;

    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool.
     * Use this to provide a list of functions the model may generate JSON inputs for.
     * A max of 128 functions are supported.
     */
    tools?: IDeepSeekToolRequest[];

    /**
     * Controls which (if any) tool is called by the model.
     *
     * none means the model will not call any tool and instead generates a message.
     *
     * auto means the model can pick between generating a message or calling one or more tools.
     *
     * required means the model must call one or more tools.
     *
     * Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool.
     *
     * none is the default when no tools are present. auto is the default if tools are present.
     */
    tool_choice?: IDeepSeekToolchoiceRequest | DeepSeekToolchoiceRequestEnum;

    /**
     * Whether to return log probabilities of the output tokens or not.
     * If true, returns the log probabilities of each output token returned
     * in the content of message.
     */
    logprobs?: boolean;

    /**
     * An integer between 0 and 20 specifying the number of most likely tokens
     * to return at each token position, each with an associated log probability.
     * logprobs must be set to true if this parameter is used.
     */
    top_logprobs?: number;
}

export interface IDeepSeekMessageRequest {
    /**
     * The contents of the role @enum {DeepSeekToolMessageRoleRequestEnum} message.
     */
    content?: string;

    /**
     * The role of the messages author
     */
    role: DeepSeekToolMessageRoleRequestEnum;

    /**
     * An optional name for the participant.
     * Provides the model information to differentiate between participants of the same role.
     */
    name?: string;

    /**
     * (Beta) Set this to true to force the model to start its answer by the content
     * of the supplied prefix in this assistant message.
     * You must set base_url="https://api.deepseek.com/beta" to use this feature.
     */
    prefix?: boolean;

    /**
     * (Beta) Used for the deepseek-reasoner model in the Chat Prefix Completion feature
     * as the input for the CoT in the last assistant message.
     * When using this feature, the prefix parameter must be set to true.
     */
    reasoning_content?: string;

    /**
     * Tool call that this message is responding to.
     */
    tool_call_id?: string;
}

export interface IDeepSeekResponseFormatRequest {
    /**
     * Must be one of text or json_object.
     */
    type: DeepSeekResponseFormatTypeRequestEnum;
}

export interface IDeepSeekStreamOptionsRequest {
    /**
     * If set, an additional chunk will be streamed before the data: [DONE] message.
     * The usage field on this chunk shows the token usage statistics for the entire request,
     * and the choices field will always be an empty array.
     * All other chunks will also include a usage field, but with a null value.
     */
    include_usage: boolean;
}

export interface IDeepSeekToolRequest {
    /**
     * The type of the tool. Currently, only function is supported.
     */
    type: DeepSeekToolCallTypeEnum;

    /**
     * The function that the model may call.
     */
    function: IDeepSeekToolFunctionRequest;
}

export interface IDeepSeekToolFunctionRequest {
    /**
     * The name of the function to be called.
     * Must be a-z, A-Z, 0-9, or contain underscores and dashes,
     * with a maximum length of 64.
     */
    name: string;

    /**
     * A description of what the function does,
     * used by the model to choose when and how to call the function.
     */
    description: string;

    /**
     * The parameters the functions accepts, described as a JSON Schema object.
     * See the Function Calling Guide for examples,
     * and the JSON Schema reference for documentation about the format.
     *
     * Omitting parameters defines a function with an empty parameter list.
     */
    parameters: IDeepSeekToolFunctionParameterRequest;
}

export interface IDeepSeekToolFunctionParameterRequest {
    /**
     * The parameters the functions accepts, described as a JSON Schema object.
     * See the Function Calling Guide for examples,
     * and the JSON Schema reference for documentation about the format.
     *
     * Omitting parameters defines a function with an empty parameter list.
     */
    'property name*': any;
}

export interface IDeepSeekToolchoiceRequest {
    /**
     * The type of the tool. Currently, only function is supported.
     */
    type: DeepSeekToolCallTypeEnum;

    /**
     * The name of the function to call.
     */
    function: IDeepSeekToolchoiceFunctionRequest;
}

export interface IDeepSeekToolchoiceFunctionRequest {
    /**
     * The name of the function to call.
     */
    name: string;
}
